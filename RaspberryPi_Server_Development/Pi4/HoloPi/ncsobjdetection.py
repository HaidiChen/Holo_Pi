# import the necessary packages
from openvino.inference_engine import IENetwork
from openvino.inference_engine import IEPlugin
from intel.yoloparams import TinyYOLOV3Params
from intel.tinyyolo import TinyYOLOv3
from imgutils.utils import Conf
import numpy as np
import imutils
import cv2
import datetime
import base64
import os


def detectFromImage(ImageFilePath):
    detectionResults = []
    # load the configuration file
    conf = Conf('config/config.json')

    # load the COCO class labels our YOLO model was trained on and
    # initialize a list of colors to represent each possible class
    # label
    LABELS = open(conf["labels_path"]).read().strip().split("\n")
    np.random.seed(42)
    COLORS = np.random.uniform(0, 255, size=(len(LABELS), 3))

    # initialize the plugin in for specified device
    plugin = IEPlugin(device="MYRIAD")

    # read the IR generated by the Model Optimizer (.xml and .bin files)
    #print("[INFO] loading models...")
    net = IENetwork(model=conf["xml_path"], weights=conf["bin_path"])

    # prepare inputs
    #print("[INFO] preparing inputs...")
    inputBlob = next(iter(net.inputs))

    # set the default batch size as 1 and get the number of input blobs,
    # number of channels, the height, and width of the input blob
    net.batch_size = 1
    (n, c, h, w) = net.inputs[inputBlob].shape

    # loading model to the plugin and start the frames per second
    # throughput estimator
    #print("[INFO] loading model to the plugin...")
    execNet = plugin.load(network=net, num_requests=1)

    # resize original frame to have a maximum width of 500 pixel and
    # input_frame to network size
    orig = cv2.imread(ImageFilePath)
    orig_copy = orig.copy()
    orig_copy = imutils.resize(orig_copy, width=500)
    frame = cv2.resize(orig_copy, (w, h))

    # change data layout from HxWxC to CxHxW
    frame = frame.transpose((2, 0, 1))
    frame = frame.reshape((n, c, h, w))

    # start inference and initialize list to collect object detection
    # results
    output = execNet.infer({inputBlob: frame})
    objects = []

    # loop over the output items
    for (layerName, outBlob) in output.items():
        # create a new object which contains the required tinyYOLOv3
        # parameters
        layerParams = TinyYOLOV3Params(net.layers[layerName].params,
            outBlob.shape[2])

        # parse the output region
        objects += TinyYOLOv3.parse_yolo_region(outBlob,
            frame.shape[2:], orig_copy.shape[:-1], layerParams,
            conf["prob_threshold"])

    # loop over each of the objects
    for i in range(len(objects)):
        # check if the confidence of the detected object is zero, if
        # it is, then skip this iteration, indicating that the object
        # should be ignored
        if objects[i]["confidence"] == 0:
            continue

        # loop over remaining objects
        for j in range(i + 1, len(objects)):
            # check if the IoU of both the objects exceeds a
            # threshold, if it does, then set the confidence of that
            # object to zero
            if TinyYOLOv3.intersection_over_union(objects[i],
                objects[j]) > conf["iou_threshold"]:
                objects[j]["confidence"] = 0

    # filter objects by using the probability threshold -- if a an
    # object is below the threshold, ignore it
    objects = [obj for obj in objects if obj['confidence'] >= \
        conf["prob_threshold"]]

    # store the height and width of the orig_copyinal frame
    (endY, endX) = orig_copy.shape[:-1]

    # loop through all the remaining objects
    for obj in objects:
        detection_dict = {}
        image_for_process = orig_copy.copy()
        # validate the bounding box of the detected object, ensuring
        # we don't have any invalid bounding boxes
        if obj["xmax"] > endX or obj["ymax"] > endY or obj["xmin"] \
            < 0 or obj["ymin"] < 0:
            continue

        # build a label consisting of the predicted class and
        # associated probability
        label = "{}: {:.2f}%".format(LABELS[obj["class_id"]],
            obj["confidence"] * 100)

        # calculate the y-coordinate used to write the label on the
        # frame depending on the bounding box coordinate
        y = obj["ymin"] - 15 if obj["ymin"] - 15 > 15 else \
            obj["ymin"] + 15

        # draw a bounding box rectangle and label on the frame
        cv2.rectangle(image_for_process, (obj["xmin"], obj["ymin"]), (obj["xmax"],
            obj["ymax"]), COLORS[obj["class_id"]], 2)
        cv2.putText(image_for_process, label, (obj["xmin"], y),
            cv2.FONT_HERSHEY_SIMPLEX, 1, COLORS[obj["class_id"]], 3)
        d = datetime.datetime.now()
        directory = 'Images/DetectionResults/' + d.date().isoformat()
        if os.path.isdir(directory) == False:
            os.mkdir(directory)

        fileName = d.time().isoformat().replace(":", "-").replace(".", "-") + '.jpg'
        filePath = os.path.join(directory, fileName)
        cv2.imwrite(filePath, image_for_process)
        with open(filePath, "rb") as fi:
            imageString = base64.b64encode(fi.read())

        detection_dict['ItemName'] = label
        detection_dict['ItemImage'] = imageString.decode()
        detection_dict['ItemDescription'] = 'This is supposed to be a description of detected item.'
        detectionResults.append(detection_dict)

    return detectionResults
